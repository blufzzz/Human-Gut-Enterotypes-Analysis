{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import SpectralClustering, DBSCAN\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import validity_index as DBCV\n",
    "from hdbscan import HDBSCAN\n",
    "from clustering_utils import clustering_by_methods, clustering, plot_clustering_scatter, metrics_formatting\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPT = 0.01 # minimal percentage of the data in single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_root = f'results/clustering_cpt-{CPT}/'\n",
    "\n",
    "if not os.path.exists(clustering_results_root):\n",
    "    os.makedirs(clustering_results_root)\n",
    "\n",
    "data_orig = {}\n",
    "for dataset_name in tqdm(['AGP', 'HMP']):\n",
    "    for tax in ['o', 'f', 'g']: \n",
    "        dataframe = pd.read_csv(f'./data_processed/{dataset_name}_{tax}.csv', sep=',')\n",
    "        label = f'{dataset_name}_{tax}'\n",
    "        data_orig[label] = dataframe.drop('Unnamed: 0', axis=1).values\n",
    "    \n",
    "data_pca = {}\n",
    "for dataset_name in tqdm(['AGP', 'HMP']):\n",
    "    for tax in ['o', 'f', 'g']: \n",
    "        label = f'{dataset_name}_{tax}'\n",
    "        data_pca[label] = np.genfromtxt(f'./results/pca/{label}', delimiter=';')\n",
    "        \n",
    "data_embeddings = defaultdict(dict)\n",
    "embeddings_path = './results/embeddings'\n",
    "    \n",
    "for emb_path in glob.glob(embeddings_path + '/*'):\n",
    "    label_info = emb_path.split('/')[-1].split('.')[0]\n",
    "    dataset, tax, embedding_type = label_info.split('_')\n",
    "    d = np.load(emb_path, allow_pickle=True).item()\n",
    "    label = dataset + '_' + tax\n",
    "    data_embeddings[embedding_type][label] = d['Z_']\n",
    "    \n",
    "distance_root = 'distances_processed'\n",
    "distances_names = ['L1', 'L2', 'JS', 'BC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_results_root = f'results/clustering_cpt-{CPT}_synthetic/'\n",
    "\n",
    "# if not os.path.exists(clustering_results_root):\n",
    "#     os.makedirs(clustering_results_root)\n",
    "\n",
    "# targets_orig = {}\n",
    "# for path in glob.glob('data/synthetic/*'):\n",
    "#     dataframe = pd.read_csv(path, index_col=0)\n",
    "#     label = path.split('/')[-1].split('.')[0]\n",
    "#     targets_orig[label] = dataframe['target'].values\n",
    "    \n",
    "# data_orig = {}\n",
    "# for path in glob.glob('data_processed/synthetic/*'):\n",
    "#     dataframe = pd.read_csv(path, index_col=0)\n",
    "#     label = path.split('/')[-1].split('.')[0]\n",
    "#     data_orig[label] = dataframe.values\n",
    "\n",
    "# data_pca = {}\n",
    "# for path in tqdm(glob.glob('./results/pca/synthetic/*')):\n",
    "#     label = path.split('/')[-1]\n",
    "#     data_pca[label] = np.genfromtxt(path, delimiter = ';')\n",
    "        \n",
    "# data_embeddings = defaultdict(dict)\n",
    "# embeddings_path = './results/embeddings_synthetic'\n",
    "# inliers_indexes_all = {}\n",
    "# for emb_path in glob.glob(embeddings_path + '/*'):\n",
    "#     label_info = emb_path.split('/')[-1].split('.')[0]\n",
    "#     dataset, tax, embedding_type = label_info.split('_')\n",
    "#     d = np.load(emb_path, allow_pickle=True).item()\n",
    "#     label = dataset + '_' + tax\n",
    "#     data_embeddings[embedding_type][label] = d['Z_']\n",
    "#     inliers_indexes_all[label_info] = d['inliers_indexes_mae']\n",
    "    \n",
    "# distance_root = 'distances_processed_synth'\n",
    "# # only L1 and L2 distances are supported\n",
    "# distances_names = ['L1', 'L2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: precomputed distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_orig = defaultdict(dict)\n",
    "for metric_name in distances_names:\n",
    "    for label in data_orig.keys(): \n",
    "        distances_orig[metric_name][label] = np.load(os.path.join(distance_root,\\\n",
    "                                                                  f'orig_{metric_name}_{label}.npy'), \n",
    "                                                     allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralClustering_prec:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.method = SpectralClustering(*args, **kwargs, affinity='precomputed', random_state=42)\n",
    "    def fit_predict(self, X):\n",
    "        '''\n",
    "        X - pairwise_distance matrix\n",
    "        '''\n",
    "        A = np.exp(-self.method.gamma * np.abs(X))\n",
    "        return self.method.fit_predict(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict_distances = {\n",
    "                           'HDBSCAN':[HDBSCAN, \n",
    "                              {'min_cluster_size':[5,10,25,50], \n",
    "                               'min_samples':[None,5,10,15,20],\n",
    "                               'metric':['precomputed'],\n",
    "                               'core_dist_n_jobs':[1],\n",
    "                               'random_state':[42]}\n",
    "                               ],\n",
    "                           'KMedoids':[KMedoids, \n",
    "                                       {'n_clusters':np.arange(2, 10, 1),\n",
    "                                        'method':['pam'],\n",
    "                                        'metric':['precomputed'],\n",
    "                                        'init':['k-medoids++'],\n",
    "                                        'random_state':[42]}],\n",
    "                           'SpectralClustering':[SpectralClustering_prec, \n",
    "                                                 {'n_clusters':np.arange(2, 10, 1), \n",
    "                                                  'gamma':[1., 5, 10, 15],\n",
    "                                                  'eigen_tol':[1e-4]}]\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for distance_name, distance_dict in tqdm(distances_orig.items()):\n",
    "    \n",
    "    results_path = os.path.join(clustering_results_root, f'{distance_name}_cluster_results')\n",
    "    \n",
    "    if os.path.isfile(results_path + '.npy'):\n",
    "        print(f'{results_path} exists, skipping...')\n",
    "        continue\n",
    "    \n",
    "    distance_cluster_results = defaultdict(dict)\n",
    "    \n",
    "    '''\n",
    "    list of dicts [\n",
    "                  dataset_1 ->  {'method1':[partition1, partition2], ...}\n",
    "                    , ...., \n",
    "                  dataset_n ->  {'method1':[partition1, partition2], ...}\n",
    "                  ]\n",
    "    \n",
    "    '''\n",
    "    clustering_data_results = Parallel(n_jobs=len(distance_dict),\n",
    "                                       mmap_mode='w+')(delayed(clustering_by_methods)(data.astype('double'), \n",
    "                                                                                      methods_dict_distances,\n",
    "                                                                                      precomputed=True,\n",
    "                                                                                      d=data_orig[label].shape[1],\n",
    "                                                                                      cluster_perc_threshold=CPT)\n",
    "                                                            for label, data in distance_dict.items())\n",
    "\n",
    "    # create compatible metrics dicts\n",
    "    for i,label in enumerate(distance_dict.keys()):\n",
    "        for j, method_name in enumerate(methods_dict_distances.keys()):\n",
    "            distance_cluster_results[label][method_name] = clustering_data_results[i][method_name]\n",
    "\n",
    "    np.save(results_path, distance_cluster_results)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {'HDBSCAN':[HDBSCAN, \n",
    "                          {'min_cluster_size':[5,10,25,50], \n",
    "                           'min_samples':[None,5,10,15,20],\n",
    "                           'core_dist_n_jobs':[1]}],\n",
    "               'KMedoids':[KMedoids, \n",
    "                           {'n_clusters':np.arange(2, 10, 1),\n",
    "                            'method':['pam'],\n",
    "                            'init':['k-medoids++'],\n",
    "                            'random_state':[42]}],\n",
    "               'SpectralClustering':[SpectralClustering, \n",
    "                                     {'n_clusters':np.arange(2, 10, 1), \n",
    "                                     'affinity':['nearest_neighbors'],\n",
    "                                     'n_neighbors': [5, 15, 25, 50], \n",
    "                                     'eigen_tol':[1e-4],\n",
    "                                     'random_state':[42]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(clustering_results_root, f'pca_cluster_results')\n",
    "\n",
    "pca_cluster_results = defaultdict(dict)\n",
    "\n",
    "clustering_data_results = Parallel(n_jobs=len(data_pca),\n",
    "                                   mmap_mode='w+')(delayed(clustering_by_methods)(data.astype('double'), \n",
    "                                                                                  methods_dict,\n",
    "                                                                                  precomputed=False,\n",
    "                                                                                  d=data.shape[1],\n",
    "                                                                                  cluster_perc_threshold=CPT)\n",
    "                                                        for label, data in data_pca.items())\n",
    "\n",
    "# create compatible metrics dicts\n",
    "for i,label in enumerate(data_pca.keys()):\n",
    "    for j, method_name in enumerate(methods_dict.keys()):\n",
    "        pca_cluster_results[label][method_name] = clustering_data_results[i][method_name]\n",
    "\n",
    "np.save(results_path, pca_cluster_results)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_type, embeddings in tqdm(data_embeddings.items()):\n",
    "    \n",
    "    results_path = os.path.join(clustering_results_root, f'{embedding_type}_cluster_results')\n",
    "    \n",
    "    if os.path.isfile(results_path + '.npy'):\n",
    "        print(f'{results_path} exists, skipping...')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'{results_path} exists, calculating...')\n",
    "    \n",
    "    embeddings_cluster_results = defaultdict(dict)\n",
    "    \n",
    "    '''\n",
    "    list of dicts [\n",
    "                  dataset_1 ->  {'method1':[partition1, partition2], ...}\n",
    "                    , ...., \n",
    "                  dataset_n ->  {'method1':[partition1, partition2], ...}\n",
    "                  ]\n",
    "    \n",
    "    '''\n",
    "    clustering_data_results = Parallel(n_jobs=len(embeddings),\n",
    "                                       mmap_mode='w+')(delayed(clustering_by_methods)(data.astype('double'), \n",
    "                                                                                      methods_dict,\n",
    "                                                                                      precomputed=False,\n",
    "                                                                                      d=data.shape[1],\n",
    "                                                                                      cluster_perc_threshold=CPT)\n",
    "                                                            for label, data in embeddings.items())\n",
    "\n",
    "    # create compatible metrics dicts\n",
    "    for i,label in enumerate(embeddings.keys()):\n",
    "        for j, method_name in enumerate(methods_dict.keys()):\n",
    "            embeddings_cluster_results[label][method_name] = clustering_data_results[i][method_name]\n",
    "\n",
    "    np.save(results_path, embeddings_cluster_results)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see paper for the thresholds explanation\n",
    "DBIND_THRESHOLD = 0.6\n",
    "DBIND_MAX = 3.\n",
    "SILHOUETTE_THRESH = 0.5\n",
    "\n",
    "PS_THRESHOLD = 0.8\n",
    "DBCV_THRESHOLD = 0.0\n",
    "\n",
    "DATA_PRESERVED_THRESHOLD = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data\n",
    "uncomment all cells in this section to use code for figures generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ['lle', 'L1', 'pca', ...]\n",
    "# representations_names = np.unique(list(map(lambda x: x.split('_')[0], os.listdir(clustering_results_root))))\n",
    "\n",
    "# clustering_results_df = []\n",
    "\n",
    "# for representation in representations_names:\n",
    "    \n",
    "#     clustreing_results_path = os.path.join(clustering_results_root, f'{representation}_cluster_results.npy')\n",
    "#     cluster_results = np.load(clustreing_results_path, allow_pickle=True).item()\n",
    "    \n",
    "#     # results for each datset\n",
    "#     for label, cl_res_label in cluster_results.items():\n",
    "#         #results for each method given dataset\n",
    "#         for cl_method, cl_res_label_method in cl_res_label.items():\n",
    "            \n",
    "#             if len(cl_res_label_method) == 0:\n",
    "#                 print('Empty results for:', representation, label, cl_method)\n",
    "#                 continue\n",
    "                \n",
    "#             df = pd.DataFrame(cl_res_label_method)\n",
    "            \n",
    "#             df['label'] = label\n",
    "#             df['representation'] = representation\n",
    "#             df['cl_method'] = cl_method\n",
    "#             # n_clusters found\n",
    "            \n",
    "#             # real clusters\n",
    "#             y = targets_orig[label]\n",
    "#             n_clusters_real = len(np.unique(y))\n",
    "#             df['n_cl_real'] = n_clusters_real - 1 \n",
    "            \n",
    "#             # calculate Adjusted Rand Index and number of clusters\n",
    "#             ari_s = []\n",
    "#             n_cl_s = []\n",
    "#             n_cl_real_s = []\n",
    "#             for cl_res_label_method_i in cl_res_label_method:\n",
    "#                 y_ = cl_res_label_method_i['labels']\n",
    "#                 mask = cl_res_label_method_i['mask']\n",
    "                \n",
    "#                 # if it is a clustering for manifold-learning embedding\n",
    "#                 if representation in data_embeddings.keys():\n",
    "#                     inliers = inliers_indexes_all[label + '_' + representation]\n",
    "#                 else:\n",
    "#                     inliers = np.arange(len(y))\n",
    "                    \n",
    "#                 n_cl  = len(np.unique(y_))\n",
    "#                 n_cl_s.append(n_cl)\n",
    "                \n",
    "#                 ari = adjusted_rand_score(y[inliers][mask], y_) \n",
    "#                 ari_s.append(ari)\n",
    "                \n",
    "#             df['ari'] = ari_s\n",
    "#             df['n_cl'] = n_cl_s\n",
    "            \n",
    "#             clustering_results_df.append(df)\n",
    "            \n",
    "# clustering_results_df = pd.concat(clustering_results_df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_results_df_ = clustering_results_df.dropna()\n",
    "# clustering_results_proper = clustering_results_df_.query('ari > 0.7 & (n_cl == n_cl_real)')\n",
    "# assert len(np.unique(clustering_results_proper['label'])) == 9 # all datasets were partitioned properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(clustering_results_proper.query(f'ps >= {PS_THRESHOLD} & dbcv > {DBCV_THRESHOLD}')['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(clustering_results_proper.query(f'dbind <= {DBIND_THRESHOLD} & silh > {SILHOUETTE_THRESH}')['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering_scatter(clustering_results_df_, \n",
    "#                         x_metric_name='dbind',\n",
    "#                         y_metric_name='silh', \n",
    "#                         coloring_metric_name='entropy', \n",
    "#                         title='Synthetic datasets clustering',\n",
    "#                         y_threshold=SILHOUETTE_THRESH,\n",
    "#                         x_threshold=DBIND_THRESHOLD,\n",
    "#                         y_hue_line=1,\n",
    "#                         x_hue_line=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = clustering_results_df_.query(f'silh >= {SILHOUETTE_THRESH} & dbind <= {DBIND_THRESHOLD} & data_used > {DATA_PRESERVED_THRESHOLD}').groupby('label')['silh'].idxmax().values\n",
    "# clustering_results_df_.loc[ind].query('n_cl_real == n_cl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering_scatter(clustering_results_df_, \n",
    "#                         x_metric_name='dbcv',\n",
    "#                         y_metric_name='ps', \n",
    "#                         coloring_metric_name='entropy', \n",
    "#                         title='Synthetic datasets clustering',\n",
    "#                         y_threshold=PS_THRESHOLD,\n",
    "#                         x_threshold=DBCV_THRESHOLD,\n",
    "#                         y_hue_line=1.2,\n",
    "#                         x_hue_line=1\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = clustering_results_df_.query(f'dbcv >= {DBCV_THRESHOLD} & ps >= {PS_THRESHOLD} & data_used > {DATA_PRESERVED_THRESHOLD}').groupby('label')['dbcv'].idxmax().values\n",
    "# clustering_results_df_.loc[ind].query('n_cl_real == n_cl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbiome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['lle', 'L1', 'pca', ...]\n",
    "representations_names = np.unique(list(map(lambda x: x.split('_')[0], os.listdir(clustering_results_root))))\n",
    "\n",
    "clustering_results_df = []\n",
    "\n",
    "for representation in representations_names:\n",
    "    \n",
    "    clustreing_results_path = os.path.join(clustering_results_root, f'{representation}_cluster_results.npy')\n",
    "    cluster_results = np.load(clustreing_results_path, allow_pickle=True).item()\n",
    "    \n",
    "    # results for each datset\n",
    "    for label, cl_res_label in cluster_results.items():\n",
    "        #results for each method given dataset\n",
    "        for cl_method, cl_res_label_method in cl_res_label.items():\n",
    "            \n",
    "            if len(cl_res_label_method) == 0:\n",
    "                print('Empty results for:', representation, label, cl_method)\n",
    "                continue\n",
    "                \n",
    "            df = pd.DataFrame(cl_res_label_method)\n",
    "            \n",
    "            dataset, tax = label.split('_')\n",
    "            \n",
    "            df['dataset'] = dataset\n",
    "            df['tax'] = tax\n",
    "            df['representation'] = representation\n",
    "            df['cl_method'] = cl_method\n",
    "            # n_clusters found\n",
    "            n_unique = lambda x: len(np.unique(x[x!=-1]))\n",
    "            df['n_cl'] = df['labels'].apply(n_unique)\n",
    "            \n",
    "            clustering_results_df.append(df)\n",
    "            \n",
    "clustering_results_df = pd.concat(clustering_results_df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask = (clustering_results_df['data_used'] > DATA_PRESERVED_THRESHOLD) &\\\n",
    "          (clustering_results_df['dbind'] < DBIND_MAX)  &\\\n",
    "          (~clustering_results_df['dbcv'].isna())\n",
    "\n",
    "clustering_results_df_ = clustering_results_df[df_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davies-Bouldin index and Silhoutte score, entropy coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20, 'axes.titley':1.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGP_df = clustering_results_df_.query('dataset == \"AGP\"')\n",
    "\n",
    "plot_clustering_scatter(AGP_df, \n",
    "                        x_metric_name='dbind',\n",
    "                        y_metric_name='silh', \n",
    "                        coloring_metric_name='entropy', \n",
    "                        title='AGP dataset clustering',\n",
    "                        y_threshold=SILHOUETTE_THRESH,\n",
    "                        x_threshold=DBIND_THRESHOLD,\n",
    "                        y_hue_line=1,\n",
    "                        x_hue_line=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_mask_AGP = (AGP_df['silh'] >= SILHOUETTE_THRESH) * (AGP_df['dbind'] <= DBIND_THRESHOLD)\n",
    "AGP_df_selected = AGP_df[selection_mask_AGP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGP_df_selected['hash'] = AGP_df_selected[['dataset','tax','representation','cl_method','n_cl']].astype(str).sum(1)\n",
    "idx = AGP_df_selected.groupby('hash')['entropy'].transform(max) == AGP_df_selected['entropy']\n",
    "AGP_df_selected[idx][['dataset', 'tax', 'representation', 'cl_method', 'n_cl', 'dbind','silh', 'dbcv', 'ps', 'entropy']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMP_df = clustering_results_df_.query('dataset == \"HMP\"')\n",
    "plot_clustering_scatter(HMP_df, \n",
    "                        x_metric_name='dbind',\n",
    "                        y_metric_name='silh', \n",
    "                        coloring_metric_name='entropy', \n",
    "                        title='HMP dataset clustering',\n",
    "                        y_threshold=SILHOUETTE_THRESH,\n",
    "                        x_threshold=DBIND_THRESHOLD,\n",
    "                        y_hue_line=1,\n",
    "                        x_hue_line=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_mask_HMP = (HMP_df['silh'] > SILHOUETTE_THRESH) * (HMP_df['dbind'] < DBIND_THRESHOLD)\n",
    "HMP_df[selection_mask_HMP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBVC and Prediction Strength, entropy coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_scatter(AGP_df, \n",
    "                        x_metric_name='dbcv',\n",
    "                        y_metric_name='ps', \n",
    "                        coloring_metric_name='entropy', \n",
    "                        title='AGP dataset clustering',\n",
    "                        y_threshold=PS_THRESHOLD,\n",
    "                        x_threshold=DBCV_THRESHOLD,\n",
    "                        y_hue_line=1.2,\n",
    "                        x_hue_line=1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_mask_AGP = (AGP_df['dbcv'] > DBCV_THRESHOLD) * (AGP_df['ps'] > PS_THRESHOLD)\n",
    "AGP_df_selected = AGP_df[selection_mask_AGP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGP_df_selected['hash'] = AGP_df_selected[['dataset','tax','representation','cl_method','n_cl']].astype(str).sum(1)\n",
    "idx = AGP_df_selected.groupby('hash')['entropy'].transform(max) == AGP_df_selected['entropy']\n",
    "AGP_df_selected[idx][['dataset', 'tax', 'representation', 'cl_method', 'n_cl', 'dbind','silh', 'dbcv', 'ps', 'entropy']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_scatter(HMP_df, \n",
    "                        x_metric_name='dbcv',\n",
    "                        y_metric_name='ps', \n",
    "                        coloring_metric_name='entropy', \n",
    "                        title='HMP dataset clustering',\n",
    "                        y_threshold=PS_THRESHOLD,\n",
    "                        x_threshold=DBCV_THRESHOLD,\n",
    "                        y_hue_line=1.2,\n",
    "                        x_hue_line=1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_mask_HMP = (HMP_df['dbcv'] > DBCV_THRESHOLD) * (HMP_df['ps'] > PS_THRESHOLD)\n",
    "HMP_df_selected = HMP_df[selection_mask_HMP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMP_df_selected['hash'] = HMP_df_selected[['dataset','tax','representation','cl_method','n_cl']].astype(str).sum(1)\n",
    "idx = HMP_df_selected.groupby('hash')['entropy'].transform(max) == HMP_df_selected['entropy']\n",
    "HMP_df_selected[idx][['dataset', 'tax', 'representation', 'cl_method', 'n_cl', 'dbind','silh', 'dbcv', 'ps', 'entropy']].round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
