{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import project_pca, calculate_Q_metrics\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://figshare.com/ndownloader/files/33927092 -O data.zip\n",
    "!unzip data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = {}\n",
    "\n",
    "for dataset_name in tqdm(['AGP', 'HMP']):\n",
    "    for tax in ['o', 'f', 'g']: \n",
    "        dataframe = pd.read_csv(f'./data/{dataset_name}/pivot_{tax}_normalized.csv', sep=';')\n",
    "        label = f'{dataset_name}_{tax}'\n",
    "        data_orig[label] = dataframe.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "processed_root = 'data_processed'\n",
    "pca_root = './results/pca/' \n",
    "os.makedirs('data_processed', exist_ok=True)\n",
    "os.makedirs(pca_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to use synthetic data\n",
    "\n",
    "# data_orig = {}\n",
    "# for path in glob.glob('data/synthetic/*'):\n",
    "#     dataframe = pd.read_csv(path, index_col=0)\n",
    "#     label = path.split('/')[-1]\n",
    "#     data_orig[label] = dataframe.drop('target', axis=1)\n",
    "\n",
    "# processed_root = 'data_processed/synthetic'\n",
    "# pca_root = './results/pca/synthetic'\n",
    "# os.makedirs(processed_root, exist_ok=True)\n",
    "# os.makedirs(pca_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement that OTU shound be present at least in RARITY_THRESHOLD percentage of population\n",
    "RARITY_THRESHOLD = 0.01 \n",
    "# requirement that OTU variance among population \n",
    "STD_THRESHOLD = 1e-3\n",
    "\n",
    "preprocessed_data = {}\n",
    "\n",
    "for label, df in data_orig.items():\n",
    "            \n",
    "    df_proc = df.copy()\n",
    "    df_proc.drop_duplicates(inplace=True)\n",
    "    N = df_proc.shape[0]\n",
    "    \n",
    "    # too rare\n",
    "    rare_otu_mask = (df_proc > 0).sum(0) / N < RARITY_THRESHOLD\n",
    "    \n",
    "    # too low std\n",
    "    std_otu_mask = df_proc.std(0) < STD_THRESHOLD\n",
    "    \n",
    "    mask = rare_otu_mask + std_otu_mask\n",
    "    df_proc = df_proc.iloc[:,~mask.values]\n",
    "    preprocessed_data[label] = df_proc\n",
    "    \n",
    "    path = os.path.join(processed_root, f'{label}.csv')\n",
    "    df_proc.to_csv(path)\n",
    "    print(f'{label} Orig shape: {df.shape}, processed shape: {df_proc.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE=True\n",
    "\n",
    "plt.figure(figsize=(7,5), dpi=300)\n",
    "plt.title(f\"Cumulative explained variance\", fontsize=20)\n",
    "plt.xlabel(\"Number of PCs\", fontsize=20)\n",
    "plt.hlines(0.99,0,300,linestyle='--')\n",
    "\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "for label, df in preprocessed_data.items():\n",
    "    \n",
    "    data_projected, pca, pca_proj, mae = project_pca(df, whiten=False)\n",
    "       \n",
    "    d_pca = data_projected.shape[1]\n",
    "    d = df.shape[1]\n",
    "    \n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_), \n",
    "            linewidth=1, \n",
    "            label=label,\n",
    "            alpha=0.6)  \n",
    "    \n",
    "    Q_loc, Q_glob = calculate_Q_metrics(df.values, data_projected)\n",
    "    \n",
    "    print(f'For {label}, dim orig: {d}, dim PCA: {d_pca}, mae: {np.round(mae,3)}, Q_loc: {np.round(Q_loc,3)}, Q_glob: {np.round(Q_glob,3)}')\n",
    "    \n",
    "    if SAVE:\n",
    "        path = os.path.join(pca_root, f'{label}')\n",
    "        np.savetxt(path, data_projected, delimiter = ';')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend(fontsize=15)       \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate pairwise distances*\n",
    "*for the faster computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import braycurtis, pdist, squareform, jensenshannon\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_root = 'distances_processed'\n",
    "os.makedirs(distance_root, exist_ok=True)\n",
    "distances_names = ['L1', 'L2', 'JS', 'BC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to use synthetic data\n",
    "\n",
    "# distance_root = 'distances_processed_synth'\n",
    "# os.makedirs(distance_root, exist_ok=True)\n",
    "# # only L1 and L2 distances are supported\n",
    "# distances_names = ['L1', 'L2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdist_l1(data, name, distance_root):\n",
    "    data_ = data.copy()\n",
    "    S = squareform(pdist(data_, metric='minkowski', p=1))\n",
    "    path = os.path.join(distance_root, f'orig_L1_{name}')\n",
    "    np.save(path, S)\n",
    "    \n",
    "_ = Parallel(n_jobs=len(preprocessed_data))(delayed(compute_pdist_l1)(data.values, name, distance_root) \n",
    "                                        for name, data in preprocessed_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdist_l2(data, name, distance_root):\n",
    "    data_ = data.copy()\n",
    "    S = squareform(pdist(data_, metric='minkowski', p=2))\n",
    "    path = os.path.join(distance_root, f'orig_L2_{name}')\n",
    "    np.save(path, S)\n",
    "    \n",
    "_ = Parallel(n_jobs=len(preprocessed_data))(delayed(compute_pdist_l2)(data.values, name, distance_root) \n",
    "                                        for name, data in preprocessed_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdist_js(data, name, distance_root):\n",
    "    \n",
    "    data_ = data.copy()\n",
    "    data_ /= data_.sum(1, keepdims=True)\n",
    "    \n",
    "    N = data_.shape[0]\n",
    "    S = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            d = jensenshannon(data_[i], data_[j])\n",
    "            S[i,j] = d\n",
    "            S[j,i] = d\n",
    "            \n",
    "    path = os.path.join(distance_root, f'orig_JS_{name}')\n",
    "    np.save(path, S)\n",
    "    \n",
    "if 'JS' in distances_names:\n",
    "    _ = Parallel(n_jobs=len(preprocessed_data))(delayed(compute_pdist_js)(data.values, name, distance_root) \n",
    "                                            for name, data in preprocessed_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdist_bc(data, name, distance_root):\n",
    "    data_ = data.copy()\n",
    "    N = data_.shape[0]\n",
    "    data_ /= data_.sum(1, keepdims=True)\n",
    "    \n",
    "    S = np.zeros((N,N))2\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            d = braycurtis(data_[i], data_[j])\n",
    "            S[i,j] = d\n",
    "            S[j,i] = d\n",
    "            \n",
    "    path = os.path.join(distance_root, f'orig_BC_{name}')\n",
    "    np.save(path, S)\n",
    "    \n",
    "if 'BC' in distances_names:\n",
    "    _ = Parallel(n_jobs=len(preprocessed_data))(delayed(compute_pdist_bc)(data.values, name, distance_root) \n",
    "                                            for name, data in preprocessed_data.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
