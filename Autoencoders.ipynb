{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import NPR,\\\n",
    "                  l1_normalized_error,\\\n",
    "                  KNN_MAE,\\\n",
    "                  calculate_Q_mae,\\\n",
    "                  l1_normalized_error_torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "                \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_dims = np.load(f'./intrinsic_dims_pca_strict.npy', allow_pickle=True).item()\n",
    "\n",
    "data_pca = {}\n",
    "for dataset_name in tqdm(['AGP', 'HMP']):\n",
    "    for tax in ['o', 'f', 'g']: \n",
    "        label = f'{dataset_name}_{tax}'\n",
    "        data_pca[label] = np.genfromtxt(f'./results/pca/{label}', delimiter=';')\n",
    "        \n",
    "data_orig = {}\n",
    "for dataset_name in tqdm(['AGP', 'HMP']):\n",
    "    for tax in ['o', 'f', 'g']: \n",
    "        dataframe = pd.read_csv(f'./data_processed/{dataset_name}_{tax}.csv', sep=',')\n",
    "        label = f'{dataset_name}_{tax}'\n",
    "        data_orig[label] = dataframe.drop('Unnamed: 0', axis=1).values\n",
    "        \n",
    "embeddings_root = f'./results/embeddings/'\n",
    "os.makedirs(embeddings_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# uncomment to use synthetic data\n",
    "\n",
    "intrinsic_dims = np.load(f'./intrinsic_dims_pca_strict_synthetic.npy', allow_pickle=True).item()\n",
    "\n",
    "data_orig = {}\n",
    "for path in glob.glob('data_processed/synthetic/*'):\n",
    "    dataframe = pd.read_csv(path, index_col=0)\n",
    "    label = path.split('/')[-1].split('.')[0]\n",
    "    data_orig[label] = dataframe.values\n",
    "\n",
    "data_pca = {}\n",
    "for path in tqdm(glob.glob('./results/pca/synthetic/*')):\n",
    "    label = path.split('/')[-1]\n",
    "    data_pca[label] = np.genfromtxt(path, delimiter = ';')\n",
    "\n",
    "    \n",
    "embeddings_root = 'results/embeddings_synthetic'\n",
    "os.makedirs(embeddings_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, config):\n",
    "        hidden_dim = config['hidden_dim']\n",
    "        dropout_rate1 = config['dropout_rate1']\n",
    "        dropout_rate2 = config['dropout_rate2']\n",
    "        dropout_rate3 = config['dropout_rate3']\n",
    "\n",
    "        dropout_rate4 = config['dropout_rate4']\n",
    "        dropout_rate5 = config['dropout_rate5']\n",
    "        dropout_rate6 = config['dropout_rate6']\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate1), \n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.BatchNorm1d(hidden_dim//2), \n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate2),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "                                    nn.BatchNorm1d(hidden_dim//4),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate3), \n",
    "                                    nn.Linear(hidden_dim//4, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(z_dim, hidden_dim//4),\n",
    "                                    nn.BatchNorm1d(hidden_dim//4),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate4),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//4, hidden_dim//2),\n",
    "                                    nn.BatchNorm1d(hidden_dim//2),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate5), \n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//2, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim), \n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate6),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Linear(hidden_dim, input_dim))\n",
    "    def forward(self, X):\n",
    "        Z = self.encoder(X)\n",
    "        X = self.decoder(Z)\n",
    "        return Z,X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(ae,\n",
    "             opt,\n",
    "             X,\n",
    "             n_epochs=6000,\n",
    "             calc_npr=True, \n",
    "             npr_calc_step=100):\n",
    "\n",
    "    X_train, X_test = train_test_split(X)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float).cuda()\n",
    "        \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float).cuda()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float).cuda()\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    results = defaultdict(list)\n",
    "    loss_val_min = np.inf\n",
    "\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        #########\n",
    "        # TRAIN #\n",
    "        #########\n",
    "        ae.train()\n",
    "        # add noise to the input\n",
    "        ae_input = X_train_tensor + 1e-2*torch.randn_like(X_train_tensor).cuda() # Add noise\n",
    "        embedding, data_rec = ae(ae_input)\n",
    "        \n",
    "        if i%npr_calc_step==0 and calc_npr:\n",
    "            results['npr_train'].append(NPR(data_train, \n",
    "                                        embedding.detach().cpu().numpy()))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = l1_normalized_error_torch(X_train_tensor, data_rec).mean() # mean of loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        results['loss_train'].append(loss.item())\n",
    "\n",
    "        ##############\n",
    "        # VALIDATION #\n",
    "        ##############\n",
    "        ae.eval()\n",
    "        with torch.no_grad():\n",
    "            embedding_, data_rec_test = ae(X_test_tensor)\n",
    "            loss_ = l1_normalized_error_torch(X_test_tensor, data_rec_test).mean()\n",
    "            results['loss_val'].append(loss_.item())\n",
    "\n",
    "            if results['loss_val'][-1] < loss_val_min:\n",
    "                loss_val_min = results['loss_val'][-1]\n",
    "                torch.save(ae.state_dict(), 'best_weights')\n",
    "            \n",
    "        if i%npr_calc_step==0 and calc_npr:\n",
    "            results['npr_val'].append(NPR(data_test, \n",
    "                                      embedding_.detach().cpu().numpy()))\n",
    "\n",
    "        # KNN MAE\n",
    "        Z, _ = ae(X_tensor)\n",
    "        results['knn_maes'].append(KNN_MAE(X,Z.detach().cpu().numpy(), \n",
    "                                           weights='distance', \n",
    "                                           averaging='median'))\n",
    "        \n",
    "    ae.load_state_dict(torch.load('best_weights'))\n",
    "    Z, _ = ae(X_tensor)\n",
    "    \n",
    "    return Z.detach().cpu().numpy(), ae, opt, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'hidden_dim': 1024,\n",
    "        'dropout_rate1': 0.05,#encoder\n",
    "        'dropout_rate2': 0.1,#encoder\n",
    "        'dropout_rate3': 0.1,#encoder\n",
    "        'dropout_rate4': 0.1,#decoder\n",
    "        'dropout_rate5': 0.1,#decoder\n",
    "        'dropout_rate6': 0.05}#decoder\n",
    "\n",
    "ae_results = {}\n",
    "ae_embeddings = {}\n",
    "\n",
    "for label,X in tqdm(data_pca.items()):\n",
    "    intrinsic_dim = intrinsic_dims[label]\n",
    "\n",
    "\n",
    "    ae = AE(X.shape[1],\n",
    "            intrinsic_dim,\n",
    "            config).cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(ae.parameters(), \n",
    "                            lr=5e-4, \n",
    "                            weight_decay=1e-5) \n",
    "\n",
    "    Z, ae, opt, results = train_ae(ae,\n",
    "                                    opt,\n",
    "                                    X,\n",
    "                                    n_epochs=5000,\n",
    "                                    calc_npr=False)\n",
    "    \n",
    "    ae_results[label] = [ae, opt, results]\n",
    "    ae_embeddings[label] = Z.detach().cpu().numpy()\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label,(ae, opt, results) in tqdm(ae_results.items()):\n",
    "    plt.figure()\n",
    "    plt.plot(results['loss_train'], label='train_loss')\n",
    "    plt.plot(results['loss_val'], label='val_loss')\n",
    "    plt.plot(results['knn_maes'], label='knn_maes')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('epoch')\n",
    "    knn_mae_min = round(min(results['knn_maes']),2)\n",
    "    loss_val_min = round(min(results['loss_val']),2)\n",
    "    plt.title(f'{label} \\n knn_min: {knn_mae_min}, loss_val_min: {loss_val_min}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mae_scorer = lambda *args: KNN_MAE(*args, averaging='median', weights='distance', n_neighbors=4)\n",
    "scorer = lambda *args: calculate_Q_mae(*args, mae_scorer=knn_mae_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGP_o\n",
      "Initial metrics: mae=0.062424180749489196, Q_loc=0.6481704333840366, Q_glob=0.8603879757550427\n",
      "After removing outliers: mae=0.057497956807831006, Q_loc=0.6638880236819423, Q_glob=0.8687097396469763\n",
      "AGP_f\n",
      "Initial metrics: mae=0.20207845967933494, Q_loc=0.5718091405008845, Q_glob=0.8162748756880082\n",
      "After removing outliers: mae=0.19135545217290187, Q_loc=0.5848797482621769, Q_glob=0.8209426600185157\n",
      "AGP_g\n",
      "Initial metrics: mae=0.22910916115806854, Q_loc=0.5886979995316357, Q_glob=0.8295949977668649\n",
      "After removing outliers: mae=0.21853174739061634, Q_loc=0.5990817629108928, Q_glob=0.8337196846015225\n",
      "HMP_o\n",
      "Initial metrics: mae=0.11370106302181365, Q_loc=0.5962449865398572, Q_glob=0.7839634692709407\n",
      "After removing outliers: mae=0.0966901854791616, Q_loc=0.6218227855677055, Q_glob=0.7986729015160193\n",
      "HMP_f\n",
      "Initial metrics: mae=0.27161362893210494, Q_loc=0.5826343225889166, Q_glob=0.7650346235998038\n",
      "After removing outliers: mae=0.24915806413046368, Q_loc=0.5930900524307338, Q_glob=0.7753092068862792\n",
      "HMP_g\n",
      "Initial metrics: mae=0.2525862250325014, Q_loc=0.5670346631712648, Q_glob=0.7694218098304398\n",
      "After removing outliers: mae=0.22558627181199595, Q_loc=0.585686485926161, Q_glob=0.7790623021520795\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 95\n",
    "DATA_PERCENT_THRESHOLD = 0.9\n",
    "\n",
    "for label,Z in ae_embeddings.items():\n",
    "    \n",
    "    print(label)\n",
    "    \n",
    "    X_orig = data_orig[label]\n",
    "    Q_loc, Q_glob, mae = scorer(X_orig, Z)\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    d['method_name'] = 'ae'\n",
    "    d['label'] = label\n",
    "    d['intrinsic_dim'] = intrinsic_dim\n",
    "    \n",
    "    d['Z'] = Z\n",
    "    d['Q_loc'] = Q_loc\n",
    "    d['Q_glob'] = Q_glob\n",
    "\n",
    "    d['knn_mae_loo_orig'] = mae\n",
    "    \n",
    "    N = X_orig.shape[0]\n",
    "    X_ = X_orig.copy()\n",
    "    Z_ = Z.copy() \n",
    "    \n",
    "    # remove outliers in embedding\n",
    "    inliers_indexes = np.arange(N)\n",
    "    while True:\n",
    "        scoring_list = KNN_MAE(X_, Z_, averaging=None, weights='distance') # distance brings nan!\n",
    "        q = np.percentile(scoring_list, PERCENTILE)\n",
    "        mask = scoring_list < q\n",
    "        if mask.sum()/N < DATA_PERCENT_THRESHOLD:\n",
    "            break\n",
    "        X_ = X_[mask]\n",
    "        Z_ = Z_[mask]\n",
    "        inliers_indexes = inliers_indexes[mask] \n",
    "\n",
    "    outliers_indexes = np.array(list(set(np.arange(N)) - set(inliers_indexes)))\n",
    "    Q_loc_, Q_glob_, mae_ = scorer(X_, Z_)\n",
    "    \n",
    "    d['inliers_indexes_mae'] = inliers_indexes\n",
    "    d['outliers_indexes_mae'] = outliers_indexes\n",
    "    d['X_'] = X_\n",
    "    d['Z_'] = Z_ # final embedding\n",
    "    d['Q_loc_'] = Q_loc_ \n",
    "    d['Q_glob_'] = Q_glob_ \n",
    "    d['knn_mae_loo_orig_'] = mae_ # final mae\n",
    "    \n",
    "    print(f'Initial metrics: mae={mae}, Q_loc={Q_loc}, Q_glob={Q_glob}') \n",
    "    print(f'After removing outliers: mae={mae_}, Q_loc={Q_loc_}, Q_glob={Q_glob_}')\n",
    "    \n",
    "    path = os.path.join(embeddings_root, f'{label}_ae')\n",
    "    np.save(path, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
